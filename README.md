# ğŸ¡ Real Estate Hybrid App

An **end-to-end Machine Learning & MLOps project** for **real estate price prediction and analytics**.

This project demonstrates how modern **data engineering**, **machine learning**, and **MLOps best practices** come together to build a **production-ready real estate system**.

---

## ğŸš€ Project Overview

The **Real Estate Hybrid App** is designed to solve real-world real estate problems by integrating multiple intelligent components into a single platform:

- ğŸ“ˆ **Price Prediction** â€“ Predict property prices using machine learning models  
- ğŸ“Š **Interactive Analytics Dashboard** â€“ Visual insights into real estate data  
- ğŸš€ **Production Deployment** â€“ Dockerized with CI/CD automation  

The project follows:
- **Cookiecutter Data Science structure**
- **MLOps best practices**
- **Scalable and maintainable architecture**

---

## ğŸ¯ Key Features

### âœ… Machine Learning
- Baseline models & advanced ensemble models  
- Hyperparameter tuning (Random Forest, LightGBM, Stacking)  
- Model evaluation & selection  
- Model registry & promotion workflow  

### âœ… Data Engineering
- Data collection via web scraping of publicly accessible real estate listings (Magicbricks, educational use only)  
- Robust data cleaning and preprocessing pipelines  
- Missing value handling and outlier treatment  
- Feature engineering for improved model performance  
- Dataset and model versioning using **DVC backed by Amazon S3**  


### âœ… Analytics & Visualization
- Exploratory Data Analysis (EDA)  
- Interactive Streamlit dashboards  
- Business-oriented insights  

### âœ… MLOps, CI/CD & Cloud Deployment
- CI/CD pipelines using GitHub Actions  
- Dockerized application  
- AWS cloud deployment  
- Model versioning & promotion  
- Automated testing  

---

## ğŸ§ª Model Development Summary

- Designed and implemented an **end-to-end machine learning pipeline** for real estate price prediction using **40K+ Magicbricks property listings**
- Applied **advanced data preprocessing, EDA, and feature engineering** techniques
- Built and optimized a **LightGBM model** using **Optuna-based hyperparameter tuning**
- Final model achieved:
  - **RÂ² â‰ˆ 0.90**
  - **MAE â‰ˆ â‚¹0.6 Cr**, demonstrating high predictive accuracy

---

## ğŸ— Project Architecture


```
real-estate-hybrid-app/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci-cd.yml                <- CI-CD pipeline (lint, test, build, Docker build & deploy) 
â”‚                              
â”‚
â”œâ”€â”€ deploy/
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ install_dependencies.sh <- Install system & Python dependencies
â”‚       â”œâ”€â”€ setup_backup.sh         <- Backup models & data
â”‚       â””â”€â”€ start_docker.sh         <- Run Docker containers
â”‚
â”œâ”€â”€ audits/
â”‚   â””â”€â”€ predictions.csv      <- Docker mount volume & security audit
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ external/       <- Third-party data
â”‚   â”œâ”€â”€ interim/        <- Cleaned intermediate data
â”‚   â”œâ”€â”€ processed/      <- Final ML-ready datasets
â”‚   â””â”€â”€ raw/            <- Original unmodified data
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ index.md
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained/
â”‚   â”œâ”€â”€ predictions/
â”‚   â””â”€â”€ registry/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ scraping_realestate_data/
â”‚   â”‚   â””â”€â”€ scrape_realestate.ipynb
â”‚   â”‚
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â”œâ”€â”€ baseline_model.ipynb
â”‚   â”œâ”€â”€ preliminary_data_cleaning.ipynb
â”‚   â”œâ”€â”€ missing_values_imputation.ipynb
â”‚   â”œâ”€â”€ locality_fix.ipynb
â”‚   â”œâ”€â”€ model_selection.ipynb
â”‚   â”œâ”€â”€ LightGBM_hp_tuning.ipynb
â”‚   â”œâ”€â”€ random_forest_hp_tuning.ipynb
â”‚   â”œâ”€â”€ dp_hp_tuning.ipynb
â”‚   â”œâ”€â”€ stacking_regression_hp_tuning.ipynb
â”‚   â””â”€â”€ eda_helper_functions.py
â”‚
â”œâ”€â”€ my_app/                         <- Streamlit App
â”‚   â”œâ”€â”€ home.py                     <- App entry point
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ Analytics.py            <- Analytics dashboard
â”‚       â””â”€â”€ Price_Predictor.py      <- Price prediction UI
â”‚
â”œâ”€â”€ src/                            <- Core ML source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                    
â”‚   â”œâ”€â”€ data_preparation.py         <- Data ingestion
â”‚   â”œâ”€â”€ data_preprocessing.py       <- preprocessing & feature engineering
â”‚   â”‚
â”‚   â””â”€â”€ modeling/
â”‚       â”œâ”€â”€ __init__.py            
â”‚       â”œâ”€â”€ train.py                <- Model training
â”‚       â”œâ”€â”€ evaluation.py           <- Model evaluation
â”‚       â””â”€â”€ model_registry.py       <- Model versioning
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_model_perf.py
â”‚   â””â”€â”€ test_model_registry.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ compose.yaml                   <- Docker Compose (with volume mounts)
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ appspec.yml                    <- Deployment configuration
â”œâ”€â”€ dvc.yaml
â”œâ”€â”€ dvc.lock
â”œâ”€â”€ params.yaml
â”œâ”€â”€ promote_model.py               <- Promote model to production
â”‚
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ requirements-docker.txt
â””â”€â”€ LICENSE
```
## ğŸ” CI/CD Pipeline (GitHub Actions)

This project uses **GitHub Actions** to implement a complete **CI/CD pipeline**, automating testing, model promotion, Docker image creation, and AWS deployment.

The pipeline is triggered **on every push** to the repository.

---

## âš™ï¸ CI/CD Workflow Overview

### ğŸ”¹ Pipeline Trigger
- The workflow runs automatically on every `git push`
- Ensures continuous integration and continuous deployment

---

## ğŸ§ª Continuous Integration (CI)

### 1ï¸âƒ£ Code Checkout
- Pulls the latest source code from the GitHub repository

### 2ï¸âƒ£ Python Environment Setup
- Uses **Python 3.12**
- Enables pip caching to speed up builds

### 3ï¸âƒ£ Dependency Installation
- Upgrades pip
- Installs development dependencies from `requirements-dev.txt`

### 4ï¸âƒ£ AWS Credentials Configuration
- Securely configures AWS credentials using GitHub Secrets
- Required for:
  - DVC S3 access
  - ECR
  - CodeDeploy

---

## ğŸ“¦ Data & Model Versioning (DVC)

### 5ï¸âƒ£ DVC Pull
- Pulls:
  - Versioned datasets
  - Trained models
- Data is fetched from **Amazon S3**, configured as the DVC remote
- Ensures reproducibility across environments

---

## ğŸ§  Model Validation & Testing

### 6ï¸âƒ£ Model Registry Test
- Validates model registration logic
- Ensures correct model versioning and metadata
- Uses secure DAGsHub authentication token

### 7ï¸âƒ£ Model Performance Test
- Checks model performance metrics
- Prevents performance regression
- Pipeline fails if model quality drops

---

## ğŸš€ Model Promotion

### 8ï¸âƒ£ Promote Model
- Executes only if all previous steps succeed
- Promotes the best-performing model to production
- Updates the model registry automatically

---

## ğŸ³ Continuous Deployment (CD)

### 9ï¸âƒ£ Amazon ECR Login
- Authenticates GitHub Actions runner with Amazon ECR
- Enables pushing Docker images securely

### ğŸ”Ÿ Docker Image Build & Push
- Builds Docker image from `Dockerfile`
- Tags image as `latest`
- Pushes image to **Amazon Elastic Container Registry (ECR)**

---

## ğŸ“¦ Deployment Packaging

### 1ï¸âƒ£1ï¸âƒ£ Create Deployment Bundle
- Packages required deployment files:
  - `appspec.yml`
  - `compose.yaml`
  - Deployment scripts
- Creates `deployment.zip` for CodeDeploy

---

## â˜ï¸ AWS Deployment

### 1ï¸âƒ£2ï¸âƒ£ Upload ZIP to Amazon S3
- Uploads deployment bundle to S3
- Acts as the source for CodeDeploy

### 1ï¸âƒ£3ï¸âƒ£ Deploy Using AWS CodeDeploy
- Triggers deployment using:
  - Application Load Balancer
  - Auto Scaling Group
- Performs rolling updates
- Ensures zero-downtime deployment

---

## ğŸ— AWS Services Used in CI/CD

- **GitHub Actions** â€“ CI/CD orchestration and automation  
- **Amazon S3** â€“ DVC remote storage and deployment artifacts  
- **Amazon ECR** â€“ Docker image registry  
- **Amazon EC2** â€“ Application hosting  
- **AWS Launch Templates** â€“ EC2 instance configuration and standardization  
- **Auto Scaling Group (ASG)** â€“ Automatic scaling and high availability  
- **Application Load Balancer (ALB)** â€“ Traffic distribution and health checks  
- **AWS CodeDeploy** â€“ In-place application deployments with rolling updates  

---

- Fully automated ML lifecycle
- Reproducible experiments using DVC
- Model quality gates before deployment
- In-place deployments with controlled rollouts
- Scalable and production-ready architecture
- Minimal to zero downtime during releases

---

This CI/CD pipeline ensures that **only validated, high-quality models** are deployed to production automatically.

## ğŸ— Cloud Architecture Overview

```mermaid
flowchart LR
    User --> ALB[Application Load Balancer]

    ALB --> ASG[Auto Scaling Group]
    ASG --> LT[AWS Launch Template]
    LT --> EC2[EC2 Instances]
    EC2 --> App[Streamlit Application]

    GitHub --> GA[GitHub Actions]
    GA --> ECR[Amazon ECR]
    GA --> S3[S3 - DVC Remote]

    ECR --> EC2
    S3 --> EC2

```


---

## ğŸ§  Why Launch Templates Are Important (Optional Explanation)

If you want a short explanation below the diagram, add this:

```markdown
### ğŸ”¹ AWS Launch Templates

- Define EC2 instance configuration:
  - AMI
  - Instance type
  - Security groups
  - IAM role
- Used by the Auto Scaling Group to launch consistent EC2 instances
- Ensures repeatable and reliable deployments
```


```markdown
## ğŸ” Security & IAM Considerations

- AWS credentials managed via **IAM roles and GitHub Secrets**
- No hard-coded secrets in codebase
- S3 access restricted using least-privilege policies
- Docker images stored in private ECR repositories
```


## ğŸ§‘â€ğŸ’» Author

**Sourav Raj**  
Data Scientist | Data Analyst

Feel free to connect on LinkedIn or explore my other projects.

